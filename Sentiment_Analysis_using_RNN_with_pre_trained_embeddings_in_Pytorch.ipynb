{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis using RNN with pre-trained embeddings in Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgN7ht63VQWf+cHcvHmXlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnli100/Sentiment_Analysis_RNN_Pretrained_embedding/blob/main/Sentiment_Analysis_using_RNN_with_pre_trained_embeddings_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9W-MsHA0mW"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpXZzAt5uCRI",
        "outputId": "e4f6ab80-1619-42b3-dccc-53642f644e00"
      },
      "source": [
        "cuda=torch.cuda.is_available()\n",
        "print(cuda)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KExhF7owVupN",
        "outputId": "2825d206-81c1-4913-c56d-967378a9505b"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install symspellpy\n",
        "!pip install inflect"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.1.7)\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (1.19.5)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXfDc5fKVH_4",
        "outputId": "e4311f94-4fcf-489d-895d-7328ec55aeae"
      },
      "source": [
        "# text pre-processing\n",
        "# inputs: \n",
        "#   remove html tags, urls\n",
        "#   remove punctuations\n",
        "#   converting numbers to corresponding words e.g. 123 to one hundred twenty-three\n",
        "#   convert to lowercase\n",
        "#   fix contractions e.g. gotta to got to, haven't to have not\n",
        "#   remove stopword e.g. the, a, i\n",
        "#   spelling correction NOTE this takes about 1 second for every 200-300 words paragraph\n",
        "#   convert to stemming word e.g. surprise to surpr\n",
        "#   lemmatize verbs e.g. walked, walking to walk\n",
        "# outputs:\n",
        "#   processed sentence\n",
        "import re\n",
        "import contractions\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "import inflect\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "# term_index is the column of the term and count_index is the column of the term frequency\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "\n",
        "def preprocess(sentence,html=True,lower=True,contraction=True,punctuation=True,convert_number=True,stopword=False,spelling=False,lemmatize=False,stem=False):\n",
        "  \n",
        "  # remove html tags, url\n",
        "  if html == True:\n",
        "    sentence = re.sub(r'<.*?>',' ',sentence)\n",
        "    sentence = re.sub(r'http\\S+',' ',sentence)\n",
        "\n",
        "  # conver to lowercases\n",
        "  if lower == True:\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "  # fix contractions e.g. gotta to got to \n",
        "  if contraction == True:\n",
        "    sentence = contractions.fix(sentence)\n",
        "\n",
        "  # remove punctuations\n",
        "  if punctuation == True:\n",
        "    #sentence = re.sub(r'[\\!\\\"\\#\\$\\%\\&\\\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\\\\\]\\^\\_\\`\\{\\|\\}\\~]',' ',sentence)\n",
        "    sentence = re.sub(r'[\\!\\\"\\#\\$\\%\\&\\\\\\(\\)\\*\\+\\,\\-\\.\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\\\\\]\\^\\_\\`\\{\\|\\}\\~]',' ',sentence)\n",
        "\n",
        "  # convert numbers into corresponding words\n",
        "  if convert_number == True:\n",
        "    p=inflect.engine()\n",
        "    sentence_converted = []\n",
        "    for w in sentence.split():\n",
        "      if w.isdigit():\n",
        "        sentence_converted.append(p.number_to_words(w))\n",
        "      else:\n",
        "        sentence_converted.append(w)\n",
        "    sentence = ' '.join(sentence_converted)\n",
        "\n",
        "  # remove stopwords e.g. the, a, i etc.\n",
        "  if stopword == True:\n",
        "    sentence = ' '.join([w for w in sentence.split() if w not in stopwords.words('english')])  \n",
        "\n",
        "  # spelling correction\n",
        "  if spelling == True:\n",
        "    # lookup suggestions for multi-word input strings (supports compound splitting & merging)\n",
        "    # max edit distance per lookup (per single word, not per whole input string)\n",
        "    corrected = sym_spell.lookup_compound(sentence, max_edit_distance=1)\n",
        "    sentence = ' '.join([str(w) for w in corrected])\n",
        "\n",
        "  # lemmatize e.g. walking, walked to walk  \n",
        "  if lemmatize == True:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sentence = ' '.join([lemmatizer.lemmatize(w) for w in sentence.split()])\n",
        "\n",
        "  # convert into stemming word e.g. surprise to surpr\n",
        "  if stem == True:\n",
        "    stemmer = LancasterStemmer()\n",
        "    sentence = ' '.join([stemmer.stem(w) for w in sentence.split()])\n",
        "\n",
        "  return sentence\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUSzQ2lK5sW6"
      },
      "source": [
        "train_data_raw,test_data_raw=torchtext.datasets.IMDB(root='.data', split=('train', 'test'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5D5tAB5sm6"
      },
      "source": [
        "train_label=[]\n",
        "train_review_raw=[]\n",
        "for l, t in train_data_raw:\n",
        "  train_label.append(l)\n",
        "  train_review_raw.append(t)  \n",
        "test_label=[]\n",
        "test_review_raw=[]\n",
        "for l, t in test_data_raw:\n",
        "  test_label.append(l)\n",
        "  test_review_raw.append(t)  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRKbHMhQ8Ibp"
      },
      "source": [
        "train_y=np.array([0 if l=='neg' else 1 for l in train_label])\n",
        "test_y=np.array([0 if l=='neg' else 1 for l in test_label])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7mRfyvJe2MK",
        "outputId": "cfd84d8f-fb1e-42af-b1b9-c7610a48f371"
      },
      "source": [
        "# pre-process reviews\n",
        "import timeit\n",
        "starttime = timeit.default_timer()\n",
        "print(\"The start time is :\",starttime)\n",
        "train_review=[preprocess(r) for r in train_review_raw]\n",
        "test_review=[preprocess(r) for r in test_review_raw]\n",
        "print(\"The time difference is :\", timeit.default_timer() - starttime)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The start time is : 3349.508231695\n",
            "The time difference is : 10.545749919999707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "57CiCf0nPKtd",
        "outputId": "9b8d30e8-b1f0-405d-ce0c-a941d3d46664"
      },
      "source": [
        "# check freq distribution of review length - 500 and less is majority\n",
        "review_len =np.zeros(len(train_review))\n",
        "for i,r in enumerate(train_review):\n",
        "  review_len[i] = len(r.split())\n",
        "plt.hist(review_len)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.7551e+04, 5.4150e+03, 1.3950e+03, 5.2400e+02, 1.0000e+02,\n",
              "        8.0000e+00, 3.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
              " array([  10. ,  257.6,  505.2,  752.8, 1000.4, 1248. , 1495.6, 1743.2,\n",
              "        1990.8, 2238.4, 2486. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq791sHTazkz"
      },
      "source": [
        "# PAD sentence less than padding number of words with '<PAD>' at beginning and truncate sentence to padding number of words to the end\n",
        "padding = 500\n",
        "train_review = [' '.join(['<PAD>'] * (padding - len(r.split())) + r.split()  if len(r.split())<padding else r.split()[:padding]) \\\n",
        "                  for r in train_review]\n",
        "test_review = [' '.join(['<PAD>'] * (padding - len(r.split())) + r.split()  if len(r.split())<padding else r.split()[:padding]) \\\n",
        "                  for r in test_review]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2LqpC9olktI",
        "outputId": "4bb9f07b-6f73-4e80-bb5b-95e0b47189df"
      },
      "source": [
        "# create counter of words in the review\n",
        "counter = Counter(' '.join(train_review+test_review).split())\n",
        "len(counter)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123052"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCqI_TrhJKSV",
        "outputId": "6587829a-fea2-44f3-80af-d6e58f588d8b"
      },
      "source": [
        "# import embedding vectors (corresponding to words) from TorchText and vocabulary linking counter words (through embedding word) to embedding vectors \n",
        "embedding_FastText = torchtext.vocab.FastText('simple')\n",
        "vocab=torchtext.vocab.Vocab(counter,vectors=embedding_FastText)\n",
        "vocab.vectors.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([123054, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFnJID1yp0Z-",
        "outputId": "2d65bf2c-a910-4d11-ccd1-e78f04ae5647"
      },
      "source": [
        "# test vocabulary\n",
        "sample_word = 'surprising'\n",
        "print(f'Word \"{vocab.itos[vocab.stoi[sample_word]]}\" has token={vocab.stoi[sample_word]} with a frequency of {vocab.freqs[sample_word]}')\n",
        "print(f'Embedding value: {vocab.vectors[vocab.stoi[sample_word]][:20]}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word \"surprising\" has token=1684 with a frequency of 573\n",
            "Embedding value: tensor([ 0.1460,  0.1829,  0.4267, -0.0951, -0.0146,  0.0448,  0.1046,  0.1308,\n",
            "        -0.1314, -0.2110, -0.1629, -0.0711, -0.3632, -0.0465, -0.2496, -0.2060,\n",
            "        -0.3353,  0.0837,  0.1147, -0.1099])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h53eKrw_yV3e",
        "outputId": "c1a81418-069b-4fc8-d2b6-554ad0cebf1d"
      },
      "source": [
        "# create embedding layer with pre-trained embedding vectors\n",
        "embedding = torch.nn.Embedding.from_pretrained(vocab.vectors,freeze=False)\n",
        "embedding(torch.LongTensor([vocab.stoi[sample_word]])).squeeze()[:20]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1460,  0.1829,  0.4267, -0.0951, -0.0146,  0.0448,  0.1046,  0.1308,\n",
              "        -0.1314, -0.2110, -0.1629, -0.0711, -0.3632, -0.0465, -0.2496, -0.2060,\n",
              "        -0.3353,  0.0837,  0.1147, -0.1099], grad_fn=<SliceBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMCLrzfytQiB"
      },
      "source": [
        "# convert processed text reviews to numpy array of tokens using the above created vocabulary\n",
        "train_x = np.array([[vocab.stoi[w] for w in r.split()] for r in train_review])\n",
        "test_x = np.array([[vocab.stoi[w] for w in r.split()] for r in test_review])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5WWtVhR3LK8"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(test_y))\n",
        "# create batches using data loader\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data,shuffle=False,batch_size=batch_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzdPEHbnYDr9",
        "outputId": "912e9d4b-b2ec-4955-ef0c-8601cc749ac1"
      },
      "source": [
        "# test dataloader\n",
        "dataiter = iter(train_loader)\n",
        "sample_x,sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 500])\n",
            "Sample input: \n",
            " tensor([[ 1445, 20873, 15707,  ...,  7002,  6376,   109],\n",
            "        [    2,     2,     2,  ...,    34,  5169,   174],\n",
            "        [    2,     2,     2,  ...,  2537,    12, 10136],\n",
            "        ...,\n",
            "        [    2,     2,     2,  ...,     9,     7,   124],\n",
            "        [    2,     2,     2,  ...,     9,    53,  1396],\n",
            "        [    2,     2,     2,  ...,    19,    22,  1393]])\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFYm-Goeyq5a"
      },
      "source": [
        "class rnn_model(nn.Module):\n",
        "  def __init__(self,embedding_dim,hidden_dim,output_dim,lstm_layers,drop_prob=0.5):\n",
        "    super(rnn_model,self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.lstm_layers = lstm_layers\n",
        "    \n",
        "    self.embeddings = nn.Embedding.from_pretrained(vocab.vectors,freeze=False)\n",
        "    self.rnn = nn.GRU(embedding_dim,hidden_dim,lstm_layers,batch_first=True,dropout=drop_prob)\n",
        "    self.fc = nn.Linear(hidden_dim,output_dim)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    batch_size = x.size(0)\n",
        "    x = self.embeddings(x.long())\n",
        "    out_rnn, hidden = self.rnn(x,hidden)\n",
        "    out = self.fc(out_rnn.contiguous().view(-1,self.hidden_dim))\n",
        "    out = self.sigmoid(out)\n",
        "    out = out.view(batch_size,-1,self.output_dim)\n",
        "    out = out[:,-1]\n",
        "    \n",
        "    return out,hidden\n",
        "  \n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = (torch.zeros(self.lstm_layers,batch_size,self.hidden_dim))\n",
        "\n",
        "    return hidden\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUsufy5jZ7On",
        "outputId": "387db5fc-59c9-4c20-c0c4-867bfe66a044"
      },
      "source": [
        "# define the RNN model\n",
        "model = rnn_model(300,100,1,1)\n",
        "print(model)\n",
        "\n",
        "# loss and optimization functions\n",
        "lr = 0.0001\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rnn_model(\n",
            "  (embeddings): Embedding(123054, 300)\n",
            "  (rnn): GRU(300, 100, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUChMMZugVT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b165c186-5f78-4662-c7e8-f7e0720fd9de"
      },
      "source": [
        "# train the model and show loss, accuracy\n",
        "batch_size=50\n",
        "n_epochs = 5\n",
        "check_accuracy = 100\n",
        "if cuda:\n",
        "  model = model.cuda()\n",
        "model.train()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  if cuda:\n",
        "    hidden = hidden.cuda()\n",
        "  for i, (inputs,labels) in enumerate(train_loader,1):\n",
        "    if cuda:\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    hidden = hidden.data\n",
        "    optimizer.zero_grad()\n",
        "    outputs, hidden = model(inputs,hidden)\n",
        "    loss = loss_function(outputs.squeeze(),labels.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "    if (i % check_accuracy) == 0:\n",
        "      val_losses = []\n",
        "      n_correct = 0\n",
        "      val_hidden = model.init_hidden(batch_size)\n",
        "      if cuda:\n",
        "        val_hidden = val_hidden.cuda()\n",
        "      model.eval()\n",
        "      for (val_inputs, val_labels) in test_loader:\n",
        "        val_hidden = val_hidden.data\n",
        "        if cuda:\n",
        "          val_inputs,val_labels = val_inputs.cuda(),val_labels.cuda()\n",
        "        val_outputs,val_hidden = model(val_inputs,val_hidden)\n",
        "        val_loss = loss_function(val_outputs.squeeze(),val_labels.float())\n",
        "        val_losses.append(val_loss.item())\n",
        "        preds = torch.round(val_outputs.squeeze())\n",
        "        correct_preds = preds.eq(val_labels.float().view_as(preds))\n",
        "        n_correct += np.sum(correct_preds.cpu().numpy().squeeze() if cuda else correct_preds.squeeze())\n",
        "      \n",
        "      model.train()\n",
        "\n",
        "      print(f'Epoch:{e+1}/{n_epochs} - batch: {i}; training loss = {loss.item()}; validation loss = {np.mean(val_losses)}; valuation accuracy = {n_correct/len(test_data)}')\n",
        "      "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1/5 - batch: 100; training loss = 0.6799650192260742; validation loss = 0.6864913156032563; valuation accuracy = 0.57092\n",
            "Epoch:1/5 - batch: 200; training loss = 0.6692885160446167; validation loss = 0.6793058961629868; valuation accuracy = 0.60048\n",
            "Epoch:1/5 - batch: 300; training loss = 0.6602709889411926; validation loss = 0.6647820892333984; valuation accuracy = 0.6324\n",
            "Epoch:1/5 - batch: 400; training loss = 0.5516782402992249; validation loss = 0.5755315380096435; valuation accuracy = 0.71796\n",
            "Epoch:1/5 - batch: 500; training loss = 0.443349689245224; validation loss = 0.4691941105425358; valuation accuracy = 0.78616\n",
            "Epoch:2/5 - batch: 100; training loss = 0.4974810779094696; validation loss = 0.44836271145939827; valuation accuracy = 0.8022\n",
            "Epoch:2/5 - batch: 200; training loss = 0.4577566087245941; validation loss = 0.419461524784565; valuation accuracy = 0.81336\n",
            "Epoch:2/5 - batch: 300; training loss = 0.35988372564315796; validation loss = 0.4089549657702446; valuation accuracy = 0.8212\n",
            "Epoch:2/5 - batch: 400; training loss = 0.4270886480808258; validation loss = 0.3842449539154768; valuation accuracy = 0.83516\n",
            "Epoch:2/5 - batch: 500; training loss = 0.35097694396972656; validation loss = 0.3767041133195162; valuation accuracy = 0.83728\n",
            "Epoch:3/5 - batch: 100; training loss = 0.2641015648841858; validation loss = 0.3719335106611252; valuation accuracy = 0.84192\n",
            "Epoch:3/5 - batch: 200; training loss = 0.3305402994155884; validation loss = 0.36182366275787353; valuation accuracy = 0.84672\n",
            "Epoch:3/5 - batch: 300; training loss = 0.1724042445421219; validation loss = 0.38880907064676284; valuation accuracy = 0.8338\n",
            "Epoch:3/5 - batch: 400; training loss = 0.14548315107822418; validation loss = 0.3535049189627171; valuation accuracy = 0.85044\n",
            "Epoch:3/5 - batch: 500; training loss = 0.24455691874027252; validation loss = 0.353098669052124; valuation accuracy = 0.85756\n",
            "Epoch:4/5 - batch: 100; training loss = 0.14866259694099426; validation loss = 0.33759342570602896; valuation accuracy = 0.8586\n",
            "Epoch:4/5 - batch: 200; training loss = 0.4201714098453522; validation loss = 0.3315246863514185; valuation accuracy = 0.86036\n",
            "Epoch:4/5 - batch: 300; training loss = 0.2247704267501831; validation loss = 0.33630670291185377; valuation accuracy = 0.86252\n",
            "Epoch:4/5 - batch: 400; training loss = 0.0858360081911087; validation loss = 0.3212817978262901; valuation accuracy = 0.86484\n",
            "Epoch:4/5 - batch: 500; training loss = 0.2252587527036667; validation loss = 0.32782632067799566; valuation accuracy = 0.86304\n",
            "Epoch:5/5 - batch: 100; training loss = 0.16072116792201996; validation loss = 0.3289132140278816; valuation accuracy = 0.85756\n",
            "Epoch:5/5 - batch: 200; training loss = 0.19064019620418549; validation loss = 0.31678311234712603; valuation accuracy = 0.87276\n",
            "Epoch:5/5 - batch: 300; training loss = 0.15886811912059784; validation loss = 0.31377059979736804; valuation accuracy = 0.87176\n",
            "Epoch:5/5 - batch: 400; training loss = 0.17875586450099945; validation loss = 0.3427926709651947; valuation accuracy = 0.86376\n",
            "Epoch:5/5 - batch: 500; training loss = 0.2961260676383972; validation loss = 0.3492332867756486; valuation accuracy = 0.8592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxEeMdG151Hb"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}